{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,sys\n",
    "import operator\n",
    "from sklearn import model_selection\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\sameer\\\\Desktop\\\\20_newsgroups'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ced64a342bcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\sameer\\Desktop\\20_newsgroups\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\sameer\\Desktop\\20_newsgroups/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\sameer\\Desktop\\20_newsgroups/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\sameer\\\\Desktop\\\\20_newsgroups'"
     ]
    }
   ],
   "source": [
    "words=[]\n",
    "target=[]\n",
    "for category in os.listdir(r\"C:\\Users\\sameer\\Desktop\\20_newsgroups\"):\n",
    "    for document in os.listdir(r'C:\\Users\\sameer\\Desktop\\20_newsgroups/'+category):\n",
    "        with open(r\"C:\\Users\\sameer\\Desktop\\20_newsgroups/\"+category+'/'+document) as file:\n",
    "            words.append((document,file.read()))\n",
    "            target.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary={}\n",
    "xtrain,xtest,ytrain,ytest=model_selection.train_test_split(words,target,test_size=0.25,random_state=0)\n",
    "from nltk.tokenize import word_tokenize       \n",
    "stop_words = set(stopwords.words('english')) #importing stopwords \n",
    "print((xtrain))\n",
    "for i in xtrain:\n",
    "    word_tokens = i[1].split()  # the zeroth index contains the folder number and first index actual text\n",
    "    filtered_sentence = [w.lower() for w in word_tokens if not w.lower() in stop_words ] # removing the stopwords\n",
    "    for i in range(0,len(filtered_sentence)):\n",
    "        if(len(filtered_sentence[i])>2):# words of length 2 and less than 2 are of no use\n",
    "            if filtered_sentence[i] in dictionary:\n",
    "                dictionary[filtered_sentence[i]]=dictionary[filtered_sentence[i]]+1\n",
    "            else:\n",
    "                dictionary[filtered_sentence[i]]=1\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_d = dict( sorted(dictionary.items(), key=operator.itemgetter(1),reverse=True))#sorting the dictionary on the basis of keys\n",
    "#print(sorted_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_words(dictionary):# returns the top k(=3000) words\n",
    "    return sorted(dictionary, key=dictionary.get, reverse=True)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_dictionary=top_k_words(dictionary)\n",
    "#print(Final_dictionary)\n",
    "len(Final_dictionary)\n",
    "index={}#creating another dictionary for further use which is just same as final_dictionary\n",
    "j=0\n",
    "for i in Final_dictionary:\n",
    "    index[i]=j\n",
    "    j=j+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print()\n",
    "l1=np.zeros((len(xtrain),2000))# creating a 2d array of len(xtrain)*3000 features since features are 3000\n",
    "#print(l1.shape)\n",
    "index1=0\n",
    "for i in xtrain:\n",
    "    word_tokens = i[1].split()\n",
    "    filtered_sentence = [w.lower() for w in word_tokens] \n",
    "    for j in range(0,len(filtered_sentence)):#filling of 2d array with the help of duplicate dictionary created earlier\n",
    "        if(filtered_sentence[j] in index):\n",
    "            l1[index1][index[filtered_sentence[j]]]= l1[index1][index[filtered_sentence[j]]]+1\n",
    "    index1=index1+1\n",
    "   \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicting(l1,ytrain,index):\n",
    "    result={}\n",
    "    all_classes=set(ytrain)\n",
    "    #print(all_classes)\n",
    "    result['total_values']=len(ytrain)\n",
    "    #l1[0]\n",
    "    for current_class in all_classes:\n",
    "        sum1=0\n",
    "        result[current_class]={}\n",
    "        current_working_rows=l1[(ytrain==current_class)]\n",
    "        #print(current_working_rows)\n",
    "        result[current_class]['total_count']=len(current_working_rows)\n",
    "        for i in index:\n",
    "            #print(i)\n",
    "            result[current_class][i]=current_working_rows[:,index[i]].sum()\n",
    "        for i in index:\n",
    "            sum1=sum1+result[current_class][i]\n",
    "        result[current_class]['total_words']=sum1\n",
    "    return result\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=predicting(l1,ytrain,index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain=[i for i in ytrain]\n",
    "ytrain=np.array(ytrain)\n",
    "ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(index,dictionary,current,x):\n",
    "    output=np.log(dictionary[current]['total_count'])-np.log(dictionary['total_values'])\n",
    "    word_tokens = x.split()\n",
    "    filtered_sentence = [w.lower() for w in word_tokens] \n",
    "    for i in range(0,len(filtered_sentence)):\n",
    "        if(filtered_sentence[i] in index):\n",
    "            count_values_with_value_xj=dictionary[current][filtered_sentence[i]]+1 #LAPLACE CORRECTION\n",
    "            count_current_class=dictionary[current]['total_words']+len(index.keys())#LAPLACE CORRECTION\n",
    "            output=(output)+np.log(count_values_with_value_xj)-np.log(count_current_class)\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_point(index,dictionary,x):\n",
    "    class_values=dictionary.keys()\n",
    "    result=-1\n",
    "    best_class=-11\n",
    "    first_run=True\n",
    "    for current in class_values:\n",
    "        if(current=='total_values'):\n",
    "            continue\n",
    "        ans=probability(index,dictionary,current,x)\n",
    "        if(first_run or ans>result):\n",
    "            result=ans\n",
    "            best_class=current\n",
    "        first_run=False\n",
    "    return best_class\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(index,result,xtest):\n",
    "    ypredictions=[]\n",
    "    for x in xtest:\n",
    "        \n",
    "        answer=predict_single_point(index,result,x[1])\n",
    "        ypredictions.append(answer)\n",
    "    return ypredictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypredictions=predict(index,result,xtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypredictions=np.array(ypredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest=[i for i in ytest]\n",
    "ytest=np.array(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "print(classification_report(ytest,ypredictions))\n",
    "print(confusion_matrix(ytest,ypredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#usinf inbuilt multimonial nb\n",
    "clf=MultinomialNB()\n",
    "clf.fit(xtrain,ytrain)\n",
    "#y_sklearn_pred=clf.predict(xtest)\n",
    "clf.score(xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
